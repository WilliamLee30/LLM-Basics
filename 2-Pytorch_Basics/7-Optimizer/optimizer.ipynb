{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prepare the data and the model'''\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading Dataset'''\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "'''Preparing your data for training with DataLoaders'''\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the Model'''\n",
    "from torch import nn\n",
    "class NeuralNetwork(nn.Module): #自定义的神经网络必须要subclass nn.Module\n",
    "    def __init__(self):\n",
    "        # 在__init__函数中初始化神经网络的layers\n",
    "        super().__init__() #千万不要忘记调用父类的__init__()函数，注册所有的参数，并且训练过程中才能进行梯度的计算和更新\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 在forward函数中就是将输入走一遍我们在__init__()中定义的网络结构，得到最后的输出\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "# 注意：\n",
    "# 1.不要直接call model.forward()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hyperparameters'''\n",
    "learning_rate = 1e-3 #在每个batch/epoch中更新模型参数的程度.学习率越小，学习的速度越慢，越大可能会导致训练过程中未知的行为。\n",
    "batch_size = 64 #在参数更新前，在网络中传播的数据样本个数\n",
    "epochs = 5 #迭代整个数据集的次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim.lr_scheduler介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loss Function'''\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.loss函数介绍\n",
    "## 1.1 常用的loss函数\n",
    "nn.MSELoss(Mean Square Error): 均方误差，通常用于回归任务<br>\n",
    "nn.NLLLoss(Negative Log Likelihood):负对数似然，用于分类任务<br>\n",
    "nn.CrossEntropyLoss结合了nn.LogSoftmax和nn.NLLLoss<br>\n",
    "\n",
    "## 1.2 torch.nn中常用的loss函数介绍\n",
    "### 1.2.1 nn.MSELoss()\n",
    "（1）定义：计算预测值和真实值之间的平方差的均值。<br>\n",
    "（2）公式：<br>\n",
    "![](./images/MSELoss的公式.png)<br>\n",
    "(3)几何意义：假设将预测值和真实值想象成二维空间中的两个点，那么MSELoss就是在计算两个点之间距离的平方的均值。最小化MSE，就等价于最小化预测点和真实点之间的距离。<br>\n",
    "（4）特点：<br>\n",
    "MSE对于离群点是很敏感的，因为在公式中，距离被平方了，所以如果有距离很大的离群点，那么会对整个loss的计算带来很大的影响。<br>\n",
    "（5）适用情况：通常用于回归任务，就是要预测房价，股价这种连续数值的任务。<br>\n",
    "\n",
    "### 1.2.2 nn.L1Loss()\n",
    "(1)定义：<br>\n",
    "计算预测值和真实值之间的绝对距离的均值。又叫做MAE Loss。<br>\n",
    "（2）数学公式：<br>\n",
    "![](./images/L1Loss的公式.png)<br>\n",
    "(3)特点：<br>\n",
    "输入的shape是（N，*），*指的是任意其他维度。输出的shape和输入一样，只不过除了batch维，就是真实值。<br>\n",
    "非负性质，L1Loss >= 0<br>\n",
    "相较于MSE Loss，MAE Loss对于离群点更加robust，因为它没有平方。<br>\n",
    "不平滑：在L1Loss = 0的地方是非平滑的，有可能导致在计算梯度的时候出现问题。<br>\n",
    "（4）适用情况：<br>\n",
    "与MSE Loss一样，适用于回归问题。<br>\n",
    "模型的Regulation项。通常在Loss中加一个L1Loss作为模型的惩罚项。主要作用是防止过拟合，对离群点鲁棒。<br>\n",
    "\n",
    "### 1.2.3 nn.BCEWithLogitsLoss()\n",
    "(1)数学推导：\n",
    "使用sigmoid激活函数，将输出的logits中的值转换为概率：<br>\n",
    "![](./images/BCELoss的公式.png)<br>\n",
    "得到输出的概率之后，就可以使用BCE Loss来计算了：<br>\n",
    "![](./images/BCELoss的公式_1.png)<br>\n",
    "(2)特点：<br>\n",
    "接受输入的形状为：(N):single-label 二分类问题；（N, C）multi-label 二分类问题。C是label的个数，label的取值仍然是0或者1。这些输入也都是模型直接输出的logits，而不能经过Softmax或者sigmoid等激活函数的处理，因为BCELoss内置了sigmoid函数的处理。输出的形状和输入是一样的，但是值为0-1标签。<br>\n",
    "（3）适用情况：<br>\n",
    "single label的二元分类：比如垃圾邮件检测问题，模型对于一个sample只输出包含一个元素的logit。<br>\n",
    "multi label的二元分类：比如图像打标签问题，一张图片可以同时被打上多个tags，（e.g., an image can be tagged as \"cat\", \"animal\", and \"outdoor\" simultaneously），每一个tag表示一个二元分类问题。模型输出的logits中对于每个tag都有一个元素值。<br>\n",
    "\n",
    "### 1.2.4 nn.CrossEntropy()\n",
    "(1)数学推导：<br>\n",
    "![](./images/CrossEntropyLoss的数学推导.png)<br>\n",
    "(2)特点：<br>\n",
    "它计算了预测的概率分布和真实分布之间的差距。通过数学推导可以看出，loss鼓励模型对于正确的分类输出更高的概率。<br>\n",
    "结合了nn.LogSoftmax()和nn.NLLLoss()，内置了softmax和负对数似然操作。<br>\n",
    "它接受(batch, class)形状的输入logits，没有经过任何处理，特别是softmax。输出的结果形状是（batch），包含了这个batch中每个sample对应类别的下标。<br>\n",
    "（3）适用情况，从推导过程来看，非常适合多分类任务。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optimizer'''\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.torch.optim常用优化器介绍\n",
    "## 1.1 torch.optim.SGD()\n",
    "(1)数学推导\n",
    "![](./images/SGD的公式.png)\n",
    "(2)语法\n",
    "```python\n",
    "torch.optim.SGD(params, lr, momentum, weight_decay) #仅仅包含了最常用的参数\n",
    "```\n",
    "(3)特殊参数介绍：\n",
    "momentum: 加速收敛，抑制震荡。通常设置为0-1之间。当设置之后，参数的更新方式就变成了下面的方式：\n",
    "![](./images/SGD加上momentum的公式.png)\n",
    "weight decay:在loss函数上加上一个L2正则化项，通过惩罚大的参数值来避免过拟合。加上之后的公式和参数更新方式变为：\n",
    "![](./images/SGD加上weightdecay的公式.png)\n",
    "![](./images/SGD加上weightdecay的公式_1.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
