{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Torch中与tensor有关的最常用属性和函数有哪些？\n",
    "![](./images/torch_tensor创建.jpg)<br>\n",
    "![](./images/torch_tensor属性.jpg)<br>\n",
    "![](./images/torch_tensor索引和切片.jpg)<br>\n",
    "![](./images/torch_tensor数据类型转换.jpg)<br>\n",
    "![](./images/torch_tensor数据shape操作.jpg)<br>\n",
    "![](./images/torch_tensor算数计算.jpg)<br>\n",
    "![](./images/torch_tensorGPU和自动求导.jpg)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "'''Initializing  a Tensor'''\n",
    "# 1.Directly from data\n",
    "data = [\n",
    "        [1, 2],\n",
    "        [3, 4]\n",
    "]\n",
    "\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(x_data)\n",
    "print(type(x_data))\n",
    "print(x_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "# 2. From a Numpy array\n",
    "np_array = np.array(data)\n",
    "print(np_array)\n",
    "print(type(np_array))\n",
    "\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "print(type(x_np))\n",
    "print(x_np.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.初始化之后的tensor的dtype是由什么决定的？\n",
    "这个问题来源于上述代码的输出中，第一个输出的tensor没有带dtype,而第二个输出中带了。\n",
    "原因：<br>\n",
    "在Pytorch的Tensor中，默认的数据类型是:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1)对于int类型：torch.int64<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2)对于float类型，torch.float32<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;而在第一个例子中，是直接从数据生成的tensor，那么数据的类型也由tensor自动推断，默认就是torch.int64类型，而默认类型，在输出tensor时，是不显示的；<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;第二个例子中，因为是先从numpy.ndarray生成的，而ndarray对于int类型的默认dtype是np.int32，因此在生成tensor时，会自动推断为torch.int32，导致不是tensor的默认数据类型，因此会显示地输出。<br>\n",
    "\n",
    "# 2.如何转换tensor和numpy的dtype？\n",
    "![tensor和ndarray的dtype转换方法](./images/tensor和ndarray的dtype转换方法.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.1039, 0.2464],\n",
      "        [0.9373, 0.6347]])\n"
     ]
    }
   ],
   "source": [
    "# 3.From another tensor\n",
    "# 新建的tensor会保留原有的属性（shape和datatype），除非显示地重写 \n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype = torch.float) #override the datatype\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6092, 0.1209, 0.3179],\n",
      "        [0.3937, 0.3992, 0.8875]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#  4.With random or constant values\n",
    "# Shpe is a tuple of tensor dimensions\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(rand_tensor)\n",
    "print(ones_tensor)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "'''Attrbutes of a Tensor'''\n",
    "tensor = torch.rand(3, 4)\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)\n",
    "\n",
    "# 移动tensor的操作，都会返回新的移动之后的tensor，而不会改变原有的tensor，这是pytorch的\n",
    "# 内部设置,是为了避免accidental overwriting, ensuring explicit control over where computations happen.\n",
    "tensor_gpu = tensor.to(\"cuda\") \n",
    "tensor_gpu_1 = tensor.cuda()\n",
    "print(tensor_gpu.device)\n",
    "print(tensor_gpu_1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 用于判断Cuda是否可用，以及GPU的设置 \n",
    "print(torch.cuda.is_available())  # True if a GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Operations on Tensors'''\n",
    "if torch.accelerator.is_available():\n",
    "    tensor = tensor.to(torch.accelerator.current_accelerator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[[0.5909, 0.2303, 0.0955, 0.9692],\n",
      "         [0.7367, 0.7870, 0.3499, 0.6610],\n",
      "         [0.6881, 0.5135, 0.5637, 0.6185],\n",
      "         [0.0351, 0.0614, 0.6553, 0.3489]],\n",
      "\n",
      "        [[0.5170, 0.7386, 0.5188, 0.9104],\n",
      "         [0.3352, 0.6535, 0.8963, 0.9468],\n",
      "         [0.6955, 0.2645, 0.5405, 0.0114],\n",
      "         [0.9743, 0.0647, 0.3262, 0.5512]],\n",
      "\n",
      "        [[0.5151, 0.0262, 0.4394, 0.6635],\n",
      "         [0.8396, 0.2170, 0.9283, 0.8664],\n",
      "         [0.5285, 0.9629, 0.5405, 0.8921],\n",
      "         [0.7249, 0.0570, 0.9306, 0.0887]]])\n",
      "tensor([[0.9692, 0.6610, 0.6185, 0.3489],\n",
      "        [0.9104, 0.9468, 0.0114, 0.5512],\n",
      "        [0.6635, 0.8664, 0.8921, 0.0887]])\n"
     ]
    }
   ],
   "source": [
    "# 1.Standard numpy-like indexing and slicing:\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor[0])\n",
    "print(tensor[:, 1])\n",
    "\n",
    "print(tensor[:, -1])\n",
    "print(tensor[..., -1])\n",
    "\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)\n",
    "\n",
    "tensor_3d = torch.rand(3, 4, 4)\n",
    "print(tensor_3d)\n",
    "print(tensor_3d[..., -1])  # Selects the last column from each 2D slice\n",
    "\n",
    "# Note:\n",
    "# \"...\" 指的是slice 前面所有的维度；\n",
    "# 当是2维数组时，tensor[:, -1]等价于tensor[..., -1]\n",
    "# 如果是3维数组，tensor[..., -1]等价于tensor[:, :, -1]\n",
    "# 如果是n维数组，tensor[..., -1]等价于tensor[:, :, 重复,:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 2.Joining tensors\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[10, 20, 30],\n",
      "         [40, 50, 60],\n",
      "         [70, 80, 90]]])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [10, 20, 30]],\n",
      "\n",
      "        [[ 4,  5,  6],\n",
      "         [40, 50, 60]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [70, 80, 90]]])\n",
      "tensor([[[ 1, 10],\n",
      "         [ 2, 20],\n",
      "         [ 3, 30]],\n",
      "\n",
      "        [[ 4, 40],\n",
      "         [ 5, 50],\n",
      "         [ 6, 60]],\n",
      "\n",
      "        [[ 7, 70],\n",
      "         [ 8, 80],\n",
      "         [ 9, 90]]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mstack((T1, T2), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mstack((T1, T2), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "# 假设是时间步T1的输出\n",
    "T1 = torch.tensor([[1, 2, 3],\n",
    "        \t\t[4, 5, 6],\n",
    "        \t\t[7, 8, 9]])\n",
    "# 假设是时间步T2的输出\n",
    "T2 = torch.tensor([[10, 20, 30],\n",
    "        \t\t[40, 50, 60],\n",
    "        \t\t[70, 80, 90]])\n",
    "\n",
    "print(torch.stack((T1, T2)))\n",
    "print(torch.stack((T1, T2), dim = 1))\n",
    "print(torch.stack((T1, T2), dim = 2))\n",
    "print(torch.stack((T1, T2), dim = 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.torch.stack()和torch.cat()的区别\n",
    "## 1.1 参考的博客链接\n",
    "https://blog.csdn.net/weixin_39504171/article/details/106074550\n",
    "https://blog.csdn.net/qq_40507857/article/details/119854085\n",
    "\n",
    "## 1.2 具体的理解-stack\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;（1）torch.stack的官方文档链接：https://pytorch.org/docs/stable/generated/torch.stack.html<br>\n",
    "stack是在一个新的dim维度，将待处理的tensor序列拼接在一起，dim的范围在0和拼接后tensor的维度数量之间；这里的维度数量指的是拼接后张量的维度个数，因为stack函数会提升张量的维度，比如二维张量stack会变成三维，三维张量stack会变为四维，n维张量stack会变为n+1维，因此对应的，拼接的时候dim的范围分别是[0,2], [0,3], [0,n-1]。<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;（2）更简单的理解，对于待拼接的k个tensor，他们的shape必须是一样的，比如都是(m, n)。那么在stack的时候，dim的范围在[0, 2]之间，-1代表最后一维，不显示指定，默认是0维；<br>\n",
    "拼接之后的tensor会变成三维的，并且在dim的索引位置插入一个新的维度，索引的上限为待拼接的tensor的个数，也就是k-1。根据这0个例子，假设dim = 0, 那么拼接之后的tensor的shape为(k, m, n), 并且concatenated_tensor[k][i][j] = 待拼接的第 k + 1 个tensor的[i][j]（这里k和k+1是因为索引和实际的顺序是差1的）；假设dim = 1, 那么拼接之后的tensor的shape为(m, k, n); 假设dim = 1, 那么拼接之后的tensor的shape为(m, n, k) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;（3）stack一般用于将多个相同形状的张量组合成一个更高维度的张量，例如将多个时间步的特征向量堆叠成一个序列张量，同时也可以用于将多个数据sample整合到一个batch中。<br>\n",
    "!注意!：上述的文字过程，实际上就是博客中的文字理解，建议搭配起来看，更容易理解。\n",
    "\n",
    "## 1.3 具体的理解-cat\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;cat的待拼接tensor的shape也是要求一致的，但是dim是在已有的某个维度上拼接，而不会产生新的维度，比如待拼接的tensor都是二维的(m, n)，那么dim的范围是[0, 1]，表示只能在已有的维度上进行合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 3.Arithmetic operations\n",
    "# 3.1 matrix multiplication\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out = y3)\n",
    "\n",
    "print(tensor)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "\n",
    "# Note:\n",
    "# 1.tensor的矩阵想乘是用 @\n",
    "# 2.矩阵的转置使用 .T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 3.2 element-wise product\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out = z3)\n",
    "\n",
    "print(tensor)\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)\n",
    "\n",
    "# Note:\n",
    "# tensor的element-wise相乘，使用的是 * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 4.Single-element tensor\n",
    "# 对于只有一个元素的tensor,可以使用 .item()函数，将tensor的数值转换为Python的数值类型\n",
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "\n",
    "agg_item = agg.item()\n",
    "print(agg_item)\n",
    "print(type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 5.In-place operations\n",
    "# 会将操作的计算结果存储回操作数的操作,叫 in-place的操作。通常使用 _下标来表示，但是这样的操作并不鼓励使用。\n",
    "# 因为虽然这样的操作能够节省内存，但是在计算导数的时候容易出现问题。\n",
    "print(tensor)\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "torch.float32\n",
      "[1. 1. 1. 1. 1.]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "'''Bridge with Numpy'''\n",
    "# 在CPU上的Tensor和Numpy的arrays能够共享内存的位置，并且改变其中一个，另外一个也会改变。\n",
    "# 1.Tensor --> Numpy Array\n",
    "t = torch.ones(5)\n",
    "print(t)\n",
    "print(t.dtype)\n",
    "\n",
    "n = t.numpy()\n",
    "print(n)\n",
    "print(n.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "float64\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 2.Numpy --> Tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(n)\n",
    "print(n.dtype)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out = n)\n",
    "\n",
    "print(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor -》 Numpy： 使用.numpy()函数；反过来使用torch.from_numpy()函数；<br>\n",
    "注意两种方法最后的返回都是一个新的变量，需要用新的变量来承接，也就是说不是in-place的操作。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
