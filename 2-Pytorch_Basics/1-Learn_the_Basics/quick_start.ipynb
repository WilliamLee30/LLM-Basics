{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import third-party libraries'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Downloading training data from open datasets'''\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = False,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = False,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data for training preparation'''\n",
    "\n",
    "batch_size = 64\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader =  DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape of X:{X.shape}\")\n",
    "    print(f\"shape and type of y:{y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'accelerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''Creaing models'''\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mcurrent_accelerator()\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241m.\u001b[39mis_avaiable() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m device\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'accelerator'"
     ]
    }
   ],
   "source": [
    "'''Creaing models'''\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_avaiable() else \"cpu\"\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''Define the layers of the network'''\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        '''Specify how data will pass through the network'''\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Instantiate a model \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define loss function and optimizer'''\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "#1. model.parameters()会返回模型所有可学习参数（就是weights和bias）的一个iterator。这些参数是torch.nn.Parameter\n",
    "# 的实例，是require_gradients = True，并且在训练过程中会更新的参数。\n",
    "# 如果想要打印模型的参数，可以使用如下的方式：\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(model.parameters())\n",
    "print(\"=\"*50)\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "# Print the number of parameter tensors\n",
    "print(f\"Number of parameter tensors: {len(params)}\")\n",
    "\n",
    "# Print each parameter's shape\n",
    "for param in params:\n",
    "    print(f\"Parameter {param.name}: Shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the model training process'''\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) #dataloader.dataset will get the original dataset object, and we \n",
    "    # can directly access the dataset through “dataloader.dataset”，比如dataloader.dataset[0]访问第一条数据\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) #计算出来的是一个 PyTorch tensor with additional attributes and functions\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print loss\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "\n",
    "# Note:\n",
    "# 1.enumerate(dataloader)会返回一个mini-batch（在实例化DataLoader对象的时候设置的batch_size）的index和这个mini-batch里的数据；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.loss常用的和函数有：\n",
    "![属性](./images/loss常用属性.jpg)<br>\n",
    "![函数](./images/loss常用函数.jpg)<br>\n",
    "\n",
    "# 2.For the optimizer:\n",
    "## 2.1why using optimizer.zero_grad() to clear gradients before next iteration ? \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;因为在pytorch中，默认是梯度的计算是累积的，这就意味着每次调用loss.backward()，那么计算出来的梯度就会被累加到模型的参数的.grad属性上，而非直接覆盖原有.grad的值；因此，如果不在下一次迭代前清除梯度的话，梯度就会持续累积，导致模型的梯度更新出错；<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;而在pytorch中，为什么又要默认支持梯度是累积的呢？<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;一方面是如果需要在多个batches之后才更新梯度的情况下，支持梯度累加之后更新，比如需要训练很大的模型，这个模型无法放到内存中？<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;另一方面是为了支持有多个loss的情况下，多次调用了loss.backward()函数计算梯度，允许最后累加起来更新参数。<br>\n",
    "\n",
    "## 2.2 针对训练大模型，需要使用累加梯度的情况的进一步说明\n",
    "（1）为什么训练大模型会需要使用累加梯度？\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;因为训练大模型的时候，通常伴随着很大的batch_size， 并且model_size会很大，就会很容易造成显存OOM的问题；<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;但是模型大小不能随意变小，而骤降batch_size会导致训练不稳定，收敛速度缓慢的问题，这个时候就可以考虑使用累加梯度，具体的使用方式的一个例子如下所示：<br>\n",
    "```python\n",
    "accumulation_steps = 4  # Accumulate gradients over 4 mini-batches\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader):\n",
    "    optimizer.zero_grad() if i % accumulation_steps == 0 else None  # Clear grads only at start\n",
    "\n",
    "    pred = model(x)  # Forward pass\n",
    "    loss = loss_fn(pred, y)  # Compute loss\n",
    "    loss.backward()  # Compute gradients (accumulate)\n",
    "\n",
    "    if (i + 1) % accumulation_steps == 0:  \n",
    "        optimizer.step()  # Update weights only after 4 mini-batches\n",
    "        optimizer.zero_grad()  # Clear gradients for next accumulation cycle\n",
    "```\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;假设原始不使用累积梯度的时候，batch_size = 64，每个batch都计算并且更新参数。保持模型大小不变，这个时候会爆显存，是因为pred = model(x)，模型在前向传播得到计算图的过程中会OOM。这个时候我们就可以把batch_size = 16，缩小为原来的四分之一，保证在一次前向传播的时候不会OOM，然后设置accumulation_steps = 4, 每个batch都计算梯度，但是每4个batch才一起更新参数，这样两者乘起来就是64，效果和原来设置batch_size = 64是一样的效果。\n",
    "\n",
    "\n",
    "## 2.3 optimier常用的属性和函数：\n",
    "![属性](./images/optimizer常用属性.jpg)<br>\n",
    "![函数](./images/optimizer常用函数.jpg)<br>\n",
    "\n",
    "\n",
    "# 3.关于model.train()和model.eval()的区别：\n",
    "## 3.1 两者的区别：\n",
    "![区别](./images/训练和验证模式的主要区别.jpg)\n",
    "## 3.2 这里BatchNorm使用的batch/learned statistics又是什么呢？<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1)batch statistics指的是在训练过程中，每个batch中计算的均值和方差，在不同的batch中，计算的数值是不一样的，因为每个batch的数据也是不一样的；<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2)learned statistics指的是使用在训练过程中，经过使用batch中计算的数据，不断更新得到的全局Running Mean & Variance，训练结束之后，就得得到了learned statistics.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(3)对于两者具体的计算公式以及区别如下：<br>\n",
    "![batch_statistics计算公式](./images/batch_statistics计算公式.jpg)<br>\n",
    "![learned_statistics计算公式](./images/learned_statistics计算公式.jpg)<br>\n",
    "![batch_learned_statistics区别](./images/batch_learned_statistics区别.jpg)<br>\n",
    "\n",
    "# 4.model常用的函数和属性有哪些？\n",
    "![model常用的属性](./images/model常用的属性.jpg)<br>\n",
    "![model常用的函数](./images/model常用的函数.jpg)<br>\n",
    "注意：model.save_state_dict()函数是不存在的，如果需要保存模型，需要用以下方式：<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;toch.save(model.state_dict(), \"model.pth\") #仅保存参数：weights & bias <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;toch.save(model, \"model.pth\") #除了参数以外，还保存了模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate the performance of the model'''\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X) \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.with torch.no_grad()的作用\n",
    "## 1.1 功能\n",
    "（1）disable 梯度计算；\n",
    "（2）节省内存和计算资源的消耗；\n",
    "## 1.2 原理\n",
    "模型不会将梯度存储在模型参数的.grad属性中。\n",
    "## 1.3 同等操作\n",
    "```python\n",
    "for param in model.parameters()\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "# 2.pred的格式是什么？\n",
    "pred是模型输出的logits，shape是(batch_size, num_classes) <br>\n",
    "```python\n",
    "# Simulating model predictions (logits) for 4 samples & 3 classes\n",
    "torch.tensor([[2.5, 1.2, 0.3],   # Class 0 (highest value at index 0)\n",
    "                     [0.1, 3.2, 2.8],   # Class 1 (highest value at index 1)\n",
    "                     [1.5, 2.1, 4.0],   # Class 2 (highest value at index 2)\n",
    "                     [2.2, 3.9, 0.5]])  # Class 1 (highest value at index 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training process'''\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch:{t + 1}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving and loading models'''\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "model =  NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.保存模型的不同方式\n",
    "## 1.1 仅仅保存模型的参数\n",
    "使用如上的方式保存和加载模型的参数，加载时需要保证模型的结构和存储的参数结构是一样的\n",
    "## 1.2 同时保存参数和模型结构\n",
    "```python\n",
    "    torch.save(model, \"model.pth\")\n",
    "    torch.load(\"model.pth\")\n",
    "    #这种方式就不需要先实例化一个未训练的模型了\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
