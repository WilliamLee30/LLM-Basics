{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.引言\n",
    "1.LLM的成功主要源自于Transformer架构，以及大量的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 什么是LLM?\n",
    "1.LLM就是用来理解，生成和回复人类文本的神经网络。<br>\n",
    "2.大模型的“大”，有两层含义：model size和dataset size。<br>\n",
    "3.大模型又被叫做generative artificial intelligence(AI), 缩写为generative AI或者GenAI.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LLM的应用有哪些？\n",
    "LLM可以应用于任何切分和生成文本的任务中，并且应用范围还在不断地被扩大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 构建LLM的阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 预训练（Pretraining）：\n",
    "1.指的是在一个large, diverse的数据集上进行训练。<br>\n",
    "2.这个数据集是原始的无标签的文本数据集。<br>\n",
    "3.预训练结束之后，模型会具备基础的两个能力：文本补全（text completion）和few-shot能力。\n",
    "（1）后者的意思是模型可以在仅有少量输入样本的情况下，在新的任务上表现良好，而无需这些新任务对应的大量训练数据。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 微调（Finetuning）：\n",
    "1.微调指的是在某个具体任务的，很小的有标签数据集上进行训练。<br>\n",
    "2.两个最常用的微调方式：指令微调（instruction-finetuning）和对分类任务的微调（finetuning for classification tasks）。<br>\n",
    "(1)指令微调就是有标签的数据包含了多个成对的指令和答案。<br>\n",
    "(2)分类微调指的是有标签数据集中包含了文本以及对应的分类标签。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 将LLM用于不同的任务\n",
    "1.现在的LLM基本都是基于Transformer架构的，因此很多时候会将LLM和Transformer混为一谈，但是要注意两点：<br>\n",
    "一是Transformer不仅可以用于LLM，而且还可以用于视觉任务；<br>\n",
    "二是LLM不完全是基于Transformer的，还有部分是基于RNN或者CNN的，这种一般是为了提高LLM的计算效率而做出的架构改变。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Encoder 和 Decoder的区别：<br>\n",
    "Transformer的encoder是用来处理输入的文本，并且将其转换为嵌入表示（embedding representation）的模块，这里的嵌入表示指的是能够在不同维度捕捉不同特征的数值表示。具体来讲，encoder会处理输入文本，并且将其编码为一系列数值表示或者向量，以捕捉输入中的上下文信息。<br>\n",
    "而decoder是将这些编码后的数值表示或者向量作为输入，将其解码并生成对应的文本输出。<br>\n",
    "encoder和decoder都是由很多layers组成的，并且这些layers通过自注意力机制(self-attention mechanism)来连接。这种自注意力机制使得模型能够捕捉输入数据中的长程依赖和上下文关系。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.BERT和GPT的区别：<br>\n",
    "（1）BERT是基于原始Transformer的encoder模块构建的，训练方式与GPT不同。<br>\n",
    "BERT主要用于掩码词汇预测（masked word prediction），意思就是在给定的句子中，预测那些被masked或者hidden的单词。<br>\n",
    "BERT一般被用于特定领域的任务，比如情感预测，和文本分类等。<br>\n",
    "（2）GPT是基于decoder的，通常被用于生成任务，特别是生成文本和文本补全。GPT这种基于decoder架构的模型，具有很强的zero-shot和few-shot能力。<br>\n",
    "前者指的是模型能够在完全没有见过的任务上泛化过去，并且不需要任何先验的特殊样本；<br>\n",
    "后者指的是模型能够从用户提供很少数量的输入样本中学习。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 使用超大的数据集\n",
    "1.与GPT 3使用的训练数据集规模类似的数据是 [The Pile](https://pile.eleuther.ai/) ，更多关于GPT 3 使用的训练数据集的讨论在[Hacker News](https://news.ycombinator.com/item?id=25607809)<br>\n",
    "\n",
    "2.GPT 3训练的成本大约是 $4.7 million。如果按照2025.3.20的汇率1美元等于7.23人民币来算，大约是3398万元。 <br>\n",
    "\n",
    "3.GPT 3 有96个transformer层和175B的参数。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 深入理解GPT的架构\n",
    "1.GPT是Generative Pretrained Transformer的缩写。<br>\n",
    "\n",
    "2.GPT是在简单的next-word prediction任务上进行的预训练。<br>\n",
    "\n",
    "3.Next word prediction指的是模型通过句子中已经出现过的词来预测新的词的学习过程。这个任务实际上是一种自监督学习（self-supervised learning）,因为我们不需要收集训练数据的标签，而是直接利用数据本身的结构，具体来讲就是使用句子或者文档中的下一个词作为模型要预测的标签。在这种情况下，我们就可以使用无标签的文本数据来训练LLM了。<br>\n",
    "\n",
    "4.GPT实际上是decoder-only架构的，每次只会预测下一个词，也被视作自回归模型（Autoregressive model）。自回归模型就是将过去的输出，作为用于预测新输出的输入。<br>\n",
    "\n",
    "5.能够在没有显示训练过的任务上表现良好的能力叫做模型的涌现能力（emergent behavior）。<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
