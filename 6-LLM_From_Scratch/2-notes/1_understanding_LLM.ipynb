{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.引言\n",
    "1.LLM的成功主要源自于Transformer架构，以及大量的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 什么是LLM?\n",
    "1.LLM就是用来理解，生成和回复人类文本的神经网络。<br>\n",
    "2.大模型的“大”，有两层含义：model size和dataset size。<br>\n",
    "3.大模型又被叫做generative artificial intelligence(AI), 缩写为generative AI或者GenAI.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LLM的应用有哪些？\n",
    "LLM可以应用于任何切分和生成文本的任务中，并且应用范围还在不断地被扩大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 构建LLM的阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 预训练（Pretraining）：\n",
    "1.指的是在一个large, diverse的数据集上进行训练。<br>\n",
    "2.这个数据集是原始的无标签的文本数据集。<br>\n",
    "3.预训练结束之后，模型会具备基础的两个能力：文本补全（text completion）和few-shot能力。\n",
    "（1）后者的意思是模型可以在仅有少量输入样本的情况下，在新的任务上表现良好，而无需这些新任务对应的大量训练数据。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 微调（Finetuning）：\n",
    "1.微调指的是在某个具体任务的，很小的有标签数据集上进行训练。<br>\n",
    "2.两个最常用的微调方式：指令微调（instruction-finetuning）和对分类任务的微调（finetuning for classification tasks）。<br>\n",
    "(1)指令微调就是有标签的数据包含了多个成对的指令和答案。<br>\n",
    "(2)分类微调指的是有标签数据集中包含了文本以及对应的分类标签。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 将LLM用于不同的任务\n",
    "1.现在的LLM基本都是基于Transformer架构的，因此很多时候会将LLM和Transformer混为一谈，但是要注意两点：<br>\n",
    "一是Transformer不仅可以用于LLM，而且还可以用于视觉任务；<br>\n",
    "二是LLM不完全是基于Transformer的，还有部分是基于RNN或者CNN的，这种一般是为了提高LLM的计算效率而做出的架构改变。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Encoder 和 Decoder的区别：<br>\n",
    "Transformer的encoder是用来处理输入的文本，并且将其转换为嵌入表示（embedding representation）的模块，这里的嵌入表示指的是能够在不同维度捕捉不同特征的数值表示。具体来讲，encoder会处理输入文本，并且将其编码为一系列数值表示或者向量，以捕捉输入中的上下文信息。<br>\n",
    "而decoder是将这些编码后的数值表示或者向量作为输入，将其解码并生成对应的文本输出。<br>\n",
    "encoder和decoder都是由很多layers组成的，并且这些layers通过自注意力机制(self-attention mechanism)来连接。这种自注意力机制使得模型能够捕捉输入数据中的长程依赖和上下文关系。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.BERT和GPT的区别：<br>\n",
    "（1）BERT是基于原始Transformer的encoder模块构建的，训练方式与GPT不同。<br>\n",
    "BERT主要用于掩码词汇预测（masked word prediction），意思就是在给定的句子中，预测那些被masked或者hidden的单词。<br>\n",
    "BERT一般被用于特定领域的任务，比如情感预测，和文本分类等。<br>\n",
    "（2）GPT是基于decoder的，通常被用于生成任务，特别是生成文本和文本补全。GPT这种基于decoder架构的模型，具有很强的zero-shot和few-shot能力。<br>\n",
    "前者指的是模型能够在完全没有见过的任务上泛化过去，并且不需要任何先验的特殊样本；<br>\n",
    "后者指的是模型能够从用户提供很少数量的输入样本中学习。<br>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
