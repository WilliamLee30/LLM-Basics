{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.建模长序列的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.RNN模型的介绍：<br>\n",
    "（1）RNN模型是一种将前序步骤的输出作为当前步骤的输入的神经网络，这种特性使得RNN适用于序列数据（比如文本）；<br>\n",
    "\n",
    "（2）RNN是一种encoder-decoder模型，当输入的文本被送进encoder之后，会进行如下的处理过程：<br>\n",
    "encoder会在每一个step都更新自己的隐藏状态（hidden state），这个隐藏状态实际上那个就是hidden layers里的值，将整个输入序列的信息都传入最终的一个隐藏状态。这个部分的作用就是将输入的整个文本处理进一个hidden state，也叫做记忆单元（memory cell）。<br>\n",
    "\n",
    "decoder就是将这一个最终的hidden state作为输入，将其用于生成输出的序列，生成的过程是一次只输出一个token的，也就是token by token的。这部分的作用就是利用encoder阶段得到的hidden state生成输出。<br>\n",
    "\n",
    "（3）RNN的缺陷：<br>\n",
    "RNN在decoder处理的阶段，无法直接访问encoder中的前序隐藏状态，而是只能依赖于encoder中得到的最终那一个hidden state（其中封装了输入序列中的所有相关信息）。这就会导致丢失上下文的信息，特别是在复杂的句子中，词与词之间的依赖关系跨越的距离非常大的情况下。<br>\n",
    "\n",
    "2.在LLM出来之前，没有注意力机制的模型有哪些问题？<br>\n",
    "正如上面介绍的RNN模型的缺陷：丢失上下文信息和无法建立长程依赖。以文本翻译为例，RNN这种逐词翻译的模型的效果就会很差，因为它在decoder阶段无法保留上下文信息和语言的语法结构。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.通过注意力机制捕捉数据依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Bahdanau attention mechanism：<br>\n",
    "这是在2014年提出的RNN的改进方法，主要的改进之处在于：<br>\n",
    "decoder可以在每一个decoding step，有选择性地去access输入序列中的任意部分token，并且在access的时候，可以通过注意力权重/分数（attention weights/scores），对输入序列中的不同token进行重要性衡量，进而对生成输出的token产生不同的重要性影响。<br>\n",
    "\n",
    "2.2017年提出的transformer就是在bahdanau注意力机制的基础上改进的，使用了self-attention机制。<br>\n",
    "\n",
    "3.Self-attention mechanism：就是在计算输入序列的表示（representation）的时候，允许输入序列中的每个位置都能够关注当前相同序列中的其他任意位置。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.使用自注意力关注输入中的不同部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.self-attention中的self指的是在一个输入序列中，通过将不同的位置关联起来，进而计算得到注意力权重。它可以评估和学习输入中各个部分之间的关系和依赖性。<br>\n",
    "\n",
    "自注意力与传统的注意力机制不一样，传统的注意力机制是关注两个不同序列中的元素之间的关系，就比如上面提到的bahdanau注意力机制就是关注的输出序列中的某个元素和输入序列中所有元素之间的关系。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 一个不包含可训练参数的简化版自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.自注意力机制self-attention的目标就是针对输入序列中的每一个元素，计算它的上下文向量(context vector)，这个向量就包括了该元素与序列中其他所有元素之间的关系信息。这里的context vector可以理解为一个enriched embedding vector，也就是tokenization和embedding之后的加强版embedding。<br>\n",
    "\n",
    "2.实现self-attention的第一步，也就是计算得到context vector的第一步。即计算注意力分数/权重：<br>\n",
    "    （1）将输入序列中当前需要计算context vector的元素，作为query，将query的embedding vector，分别与属于序列中的每一个元素的embedding vector做点积(dot product)，包括当前元素本身也要计算，得到注意力分数；<br>\n",
    "- 使用点积的愿意：点积的结果是一个常数，方便作为权重；点积也是一个用于衡量两个向量之间的相似度的方法，能够量化两个向量之间的对齐程度，结果越大表明两个向量之间越相似；在自注意力机制中，序列中元素之间的embedding vector的点积结果就表明了它们之间互相关注的程度，点积结果越大，表明两个元素之间的相似性越高。<br>\n",
    "\n",
    "（2）在得到的注意力分数的基础上，应用归一化(normalization)方法对其进行归一化，得到最终的注意力权重（和为1），通常使用的是Softmax函数。<br>\n",
    "- 归一化的目的就是让注意力权重的为一，这对LLM的解释性和训练过程中的训练稳定性有帮助；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
